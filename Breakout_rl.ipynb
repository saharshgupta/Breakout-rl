{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Breakout-rl.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vXh21ZxIO3xh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This project is for a Atari game playing agent, using reinforcement learning.\n",
        "This project follows the Deepmind research paper, \"Playing Atari with Deep Reinforcement Learning\"\n",
        "Link: https://arxiv.org/pdf/1312.5602v1.pdf\n",
        "Refered to: https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26 \n",
        "and\n",
        "https://github.com/boyuanf/DeepQLearning"
      ]
    },
    {
      "metadata": {
        "id": "rZWNneitunSq",
        "colab_type": "code",
        "outputId": "69e1568f-f48f-453a-f778-f58a3db200eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# @title Importing required Libraries\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.transform import resize\n",
        "from keras.models import Model\n",
        "\n",
        "from collections import deque\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from keras.models import clone_model\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.losses import mse"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "qoG1TpDNu8WE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @title Hyper-parameters required\n",
        "I_EPSILON = 1\n",
        "F_EPSILON = 0.1\n",
        "EPSILON_STEPS = 100000\n",
        "BATCH_SIZE = 32\n",
        "GAMMA = 0.99\n",
        "NUM_EPISODES = 2000\n",
        "REPLAY_MEMORY = 200000\n",
        "RENDER = False\n",
        "ATARI_SHAPE = (84,84,4)\n",
        "ACTION_SIZE = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_q25QxFdvKiZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @title Preprocessing and model\n",
        "# @markdown Preprocessing the Observation, converting to grayscale and resizing.\n",
        "def preprocess(observation):\n",
        "  processed_observation = np.uint8(resize(rgb2gray(observation),(84,84),mode=\"constant\")*255)\n",
        "  return processed_observation\n",
        "\n",
        "# @markdown Creating DQN using keras functional API.\n",
        "def model():\n",
        "  # Using functional API of keras\n",
        "  input_frames = layers.Input(ATARI_SHAPE,name=\"input_frames\")\n",
        "  input_actions = layers.Input((ACTION_SIZE,),name=\"action\")\n",
        "  \n",
        "  normalized = layers.Lambda(lambda x: x/255.0, name=\"normalized\")(input_frames)\n",
        "  \n",
        "  # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
        "  conv_1 = layers.convolutional.Conv2D(16, (8, 8), strides=(4, 4), activation='relu')(normalized)\n",
        "  \n",
        "  # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed by a rectifier nonlinearity.\"\n",
        "  conv_2 = layers.convolutional.Conv2D(32, (4, 4), strides=(2, 2), activation='relu')(conv_1)\n",
        "    \n",
        "  # Flattening the second convolutional layer.\n",
        "  conv_flattened = layers.core.Flatten()(conv_2)\n",
        "  \n",
        "  # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
        "  hidden = layers.Dense(256, activation='relu')(conv_flattened)\n",
        "  \n",
        "  # \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
        "  output = layers.Dense(ACTION_SIZE)(hidden)\n",
        "  \n",
        "  # Finally, we multiply the output by the mask!\n",
        "  filtered_output = layers.Multiply(name='QValue')([output, input_actions])\n",
        "\n",
        "  model = Model(inputs=[input_frames, input_actions], outputs=filtered_output)\n",
        "  model.summary()\n",
        "  optimizer = RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
        "  model.compile(optimizer, loss=mse)\n",
        "  return model\n",
        "\n",
        "# @markdown Using epsilon-greedy policy select an action\n",
        "def get_action(model,epsilon,history):\n",
        "  if np.random.rand() <= epsilon:\n",
        "    return random.randrange(ACTION_SIZE)\n",
        "  else:\n",
        "    q_value = model.predict([history,np.ones(ACTION_SIZE).reshape(1,ACTION_SIZE)])\n",
        "    return np.argmax(q_value[0])\n",
        " \n",
        "\n",
        "# @markdown Update memory for experience replay\n",
        "def exp_replay(exp,history,action,reward,next_history,dead):\n",
        "  exp.append((history,action,reward,next_history,dead))\n",
        "  \n",
        " \n",
        "# @markdown Get one-hot \n",
        "def one_hot(targets,nb_classes):\n",
        "  return np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
        "\n",
        "\n",
        "# @markdown Training mini batches\n",
        "def train_batch(model,exp):\n",
        "  mini_batch = random.sample(exp, BATCH_SIZE)\n",
        "  history = np.zeros((BATCH_SIZE,ATARI_SHAPE[0],ATARI_SHAPE[1],ATARI_SHAPE[2]))\n",
        "  next_history = np.zeros((BATCH_SIZE,ATARI_SHAPE[0],ATARI_SHAPE[1],ATARI_SHAPE[2]))\n",
        "  target = np.zeros((BATCH_SIZE,))\n",
        "  \n",
        "  action,reward,dead = [], [], []\n",
        "  \n",
        "  for i, val in enumerate(mini_batch):\n",
        "    history[i] = val[0]\n",
        "    action.append(val[1])\n",
        "    reward.append(val[2])\n",
        "    next_history[i] = val[3]\n",
        "    dead.append(val[4])\n",
        "    \n",
        "  actions = np.ones((BATCH_SIZE,ACTION_SIZE))\n",
        "  next_q_values = model.predict([next_history,actions])\n",
        "  \n",
        "  for i in range(BATCH_SIZE):\n",
        "    # terminal target is the reward\n",
        "    if dead:\n",
        "      target[i]=reward[i]\n",
        "    # Non-terminal the target is the discounted reward\n",
        "    else:\n",
        "      target[i]=reward[i]+GAMMA*np.amax(next_q_values[i])\n",
        "      \n",
        "  action_one_hot = one_hot(action ,ACTION_SIZE)\n",
        "  target_one_hot = action_one_hot * target[:,None]\n",
        "  \n",
        "  fitted_model = model.fit([history,action_one_hot],target_one_hot,epochs=1,batch_size=BATCH_SIZE,verbose=0)\n",
        "  \n",
        "  return fitted_model.history['loss'][0]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pShBd3NJIpGj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @title Training the model\n",
        "# @markdown environment = BreakoutDeterministic-v4\n",
        "def train():\n",
        "  env = gym.make('BreakoutDeterministic-v4')\n",
        "  experience = deque(maxlen=REPLAY_MEMORY)\n",
        "  episode_number = 0\n",
        "  epsilon = I_EPSILON\n",
        "  epsilon_decay = (I_EPSILON-F_EPSILON)/EPSILON_STEPS\n",
        "  target_model = model()\n",
        "  while episode_number <= NUM_EPISODES:\n",
        "    done = False\n",
        "    dead = False\n",
        "    step, score, start_life = 0, 0, 5\n",
        "    loss = 0.0\n",
        "    observe = env.reset()\n",
        "    \n",
        "    # taking a random step at the start of the episode\n",
        "    observe, _, _, _ = env.step(1)\n",
        "    \n",
        "    state = preprocess(observe)\n",
        "    history = np.stack((state,state,state,state),axis=2)\n",
        "    history = np.reshape([history],(1,84,84,4))\n",
        "    \n",
        "    while not done:\n",
        "      if epsilon > F_EPSILON and episode_number > 320:\n",
        "        epsilon -= epsilon_decay\n",
        "        \n",
        "      action = get_action(target_model,epsilon,history)\n",
        "      \n",
        "      observe, reward, done, info = env.step(action)\n",
        "      \n",
        "      next_state = preprocess(observe)\n",
        "      next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
        "      next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
        "      \n",
        "      if start_life > info['ale.lives']:\n",
        "        dead = True\n",
        "        start_life = info['ale.lives']\n",
        "        \n",
        "      exp_replay(experience,history,action,reward,next_history,dead)\n",
        "      \n",
        "      if episode_number > 320:\n",
        "        loss = loss + train_batch(target_model, experience)\n",
        "        \n",
        "      \n",
        "      score += reward\n",
        "      \n",
        "      if dead:\n",
        "        dead = False\n",
        "      else:\n",
        "        history = next_history\n",
        "\n",
        "      step += 1\n",
        "      \n",
        "      if done:\n",
        "        print('episode: {}, score: {}, avg loss: {}, step: {}, Replay length: {}'\n",
        "              .format(episode_number, score, loss / float(step), step, len(experience)))\n",
        "        episode_number += 1\n",
        "     \n",
        "  target_model.save(\"trained_model.h5\")\n",
        "  env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O6MwFFK5OGqn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OyuKrS8JOYKE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @title Testing the agent for 10 Episodes\n",
        "def test():\n",
        "    env = gym.make('BreakoutDeterministic-v4')\n",
        "\n",
        "    episode_number = 0\n",
        "    epsilon = 0.001\n",
        "    model = load_model(\"trained_model.h5\")\n",
        "    while episode_number < 10:\n",
        "\n",
        "        done = False\n",
        "        dead = False\n",
        "        # 1 episode = 5 lives\n",
        "        score, start_life = 0, 5\n",
        "        observe = env.reset()\n",
        "\n",
        "        observe, _, _, _ = env.step(1)\n",
        "        # At start of episode, there is no preceding frame\n",
        "        # So just copy initial states to make history\n",
        "        state = preprocess(observe)\n",
        "        history = np.stack((state, state, state, state), axis=2)\n",
        "        history = np.reshape([history], (1, 84, 84, 4))\n",
        "\n",
        "        while not done:\n",
        "            #env.render()\n",
        "\n",
        "            # get action for the current history and go one step in environment\n",
        "            action = get_action(model, epsilon, history)\n",
        "\n",
        "            observe, reward, done, info = env.step(action)\n",
        "            # pre-process the observation --> history\n",
        "            next_state = preprocess(observe)\n",
        "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
        "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
        "\n",
        "            # if the agent missed ball, agent is dead --> episode is not over\n",
        "            if start_life > info['ale.lives']:\n",
        "                dead = True\n",
        "                start_life = info['ale.lives']\n",
        "\n",
        "            score += reward\n",
        "\n",
        "            # If agent is dead, set the flag back to false, but keep the history unchanged,\n",
        "            # to avoid to see the ball up in the sky\n",
        "            if dead:\n",
        "                dead = False\n",
        "            else:\n",
        "                history = next_history\n",
        "\n",
        "            if done:\n",
        "                episode_number += 1\n",
        "                print('episode: {}, score: {}'.format(episode_number, score))\n",
        "    env.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "57z_rLHnaQbr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WX1WweGYaRkg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Create & upload a text file.\n",
        "model_file = drive.CreateFile({'title' : 'trained_model.h5'})\n",
        "model_file.SetContentFile('trained_model.h5')\n",
        "model_file.Upload()\n",
        "print('Uploaded file with ID {}'.format(model_file.get('id')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mBK1bzeby8Yu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}